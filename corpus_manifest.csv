source_id,title,authors,year,type,venue,link,relevance
S01,"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions","Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu",2023,Survey (peer-reviewed),ACM Transactions on Information Systems (TOIS),https://arxiv.org/abs/2311.05232,Foundational survey defining factuality vs. faithfulness hallucination taxonomy. Provides conceptual framework for evaluating when LLM outputs diverge from provided context.
S02,FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation,"Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi",2023,Benchmark paper (peer-reviewed),EMNLP 2023,https://arxiv.org/abs/2305.14251,Introduces atomic fact decomposition for measuring factual precision. Automated estimator approach informs evaluation methodology.
S03,TruthfulQA: Measuring How Models Mimic Human Falsehoods,"Stephanie Lin, Jacob Hilton, Owain Evans",2022,Benchmark paper (peer-reviewed),ACL 2022,https://arxiv.org/abs/2109.07958,Key benchmark for LLM truthfulness. Finding that larger models are often less truthful (inverse scaling) informs Sub-Q A. Provides adversarial question design methodology.
S04,Fine-grained Hallucination Detection and Editing for Language Models,"Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi",2024,Empirical study (peer-reviewed),COLM 2024,https://arxiv.org/abs/2401.06855,Introduces 6-type hallucination taxonomy and FavaBench benchmark. Shows ChatGPT and Llama2 hallucinate in 60-75% of information-seeking outputs.
S05,Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,"Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings",2024,Empirical study (peer-reviewed),EMNLP 2024 Findings,https://aclanthology.org/2024.findings-emnlp.882.pdf,Measures whether CoT reasoning steps actually influence model outputs via causal mediation analysis. Relevant to Sub-Q A and C.
S06,On Assessing the Faithfulness of LLM-generated Feedback on Student Assignments,"Qinjin Jia, Jialin Cui, Ruijie Xi, Chengyuan Liu, Parvez Rashid, Ruochi Li, Edward Gehringer",2024,Empirical study (peer-reviewed),EDM 2024,https://files.eric.ed.gov/fulltext/ED675643.pdf,Compares data-driven (27.1% hallucination) vs. prompt-driven (23.5%) feedback systems. Intrinsic hallucinations dominate fine-tuned models; extrinsic dominate prompt-driven. Best hallucination measurement F1 ≈ 72%.
S07,Enabling Large Language Models to Generate Text with Citations,"Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen",2023,Benchmark paper (peer-reviewed),EMNLP 2023,https://arxiv.org/abs/2305.14627,First reproducible benchmark (ALCE) for evaluating LLM citation quality. Defines citation recall and precision metrics. Best models lack complete citation support 50% of the time.
S08,Measuring Attribution in Natural Language Generation Models,"Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, David Reitter",2023,Framework paper (peer-reviewed),"Computational Linguistics, Vol. 49, No. 4 (MIT Press)",https://arxiv.org/abs/2112.12870,Defines the Attributable to Identified Sources (AIS) framework. Provides annotation guidelines and two-stage evaluation pipeline.
S09,Evaluating Verifiability in Generative Search Engines,"Nelson F. Liu, Tianyi Zhang, Percy Liang",2023,Empirical study (peer-reviewed),EMNLP 2023 Findings,https://arxiv.org/abs/2304.09848,"Audits citation recall/precision in commercial systems (Bing Chat, Perplexity). Only 51.5% of statements fully supported by citations."
S10,Chain-of-Thought Improves Text Generation with Citations in Large Language Models,(Authors from AAAI 2024 proceedings),2024,Empirical study (peer-reviewed),AAAI 2024,https://ojs.aaai.org/index.php/AAAI/article/view/29794,Tests CoT prompting for citation generation on ALCE benchmark across 6 LLMs. CoT consistently improves citation precision and recall.
S11,A Survey of Large Language Models Attribution,(Authors from HITsz-TMG),2023,Survey,arXiv preprint (arXiv:2311.03731),https://arxiv.org/abs/2311.03731,"Covers pre-generation, in-generation, and post-generation attribution approaches. Provides taxonomy of attribution systems."
S12,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Fei Xia, Quoc Le, Denny Zhou",2022,Empirical study (peer-reviewed),NeurIPS 2022,https://arxiv.org/abs/2201.11903,Foundational CoT prompting paper. Demonstrates intermediate reasoning steps improve performance on complex tasks.
S13,Survey and Analysis of Hallucinations in Large Language Models: Attribution to Prompting Strategies or Model Behavior,"(PMC/NIH published, multi-author)",2025,Empirical study (peer-reviewed),PMC (peer-reviewed journal),https://pmc.ncbi.nlm.nih.gov/articles/PMC12518350/,Empirically separates prompt-induced hallucinations from model-intrinsic ones. Tests multiple LLMs with standardized benchmarks and proposes diagnostic framework.
S14,A Comprehensive Survey on Trustworthiness in Reasoning,"(Multi-author, OpenReview)",2024,Survey (peer-reviewed),OpenReview (submitted for peer review),https://openreview.net/pdf?id=Ysslwdjb6L,Distinguishes between answers being correct vs. reasoning process being faithful. Clarifies faithfulness definitions in the literature.
S15,Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting,"Lennart Meincke, Ethan R. Mollick, Lilach Mollick, Dan Shapiro",2025,Technical report,The Wharton School Research Paper (SSRN),https://ssrn.com/abstract=5285532,Shows CoT effectiveness varies by model type and task. Reasoning models gain minimal benefit from explicit CoT.
S16,Towards Faithful Model Explanation in NLP: A Survey,Qing Lyu et al.,2024,Survey (peer-reviewed),Computational Linguistics (ACL),https://aclanthology.org/2024.cl-2.6.pdf,Reviews 110+ explanation methods through the lens of faithfulness. Provides rigorous definitions and evaluation principles.
S17,Large Language Models Hallucination: A Comprehensive Survey,"Aisha Alansari, Hamzah Luqman",2025,Survey,arXiv preprint (arXiv:2510.06265),https://arxiv.org/abs/2510.06265,Most recent comprehensive hallucination survey covering full LLM development lifecycle. Updated taxonomy of detection and mitigation approaches.
S18,Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations,(OpenReview submission),2024,Empirical study,OpenReview,https://openreview.net/forum?id=4ub9gpx9xw,Measures explanation faithfulness by testing whether concepts LLMs claim are influential actually are.
S19,A Comprehensive Survey of Faithfulness Evaluation Methods,(RANLP 2025 proceedings),2025,Survey (peer-reviewed),RANLP 2025,https://acl-bg.org/proceedings/2025/RANLP%202025/pdf/2025.ranlp-1.74.pdf,"Surveys faithfulness evaluation methods: fact-based, classifier-based, QA-based, and LLM-based approaches."
S20,The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models,"Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",2024,Empirical study (peer-reviewed),ACL 2024,https://aclanthology.org/2024.acl-long.586/,"HaluEval 2.0 benchmark (8,770 questions, 5 domains). 6-type factuality hallucination taxonomy. Retrieval augmentation reduces hallucinations; CoT helps larger models but hurts smaller ones."
S21,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,"Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",2023,Benchmark paper (peer-reviewed),EMNLP 2023,https://aclanthology.org/2023.emnlp-main.397.pdf,"35,000-sample hallucination evaluation benchmark across QA, dialogue, summarization. ChatGPT hallucinates ~19.5% of responses. Knowledge retrieval boosts recognition accuracy."
S22,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",2020,Foundational paper (peer-reviewed),NeurIPS 2020,https://arxiv.org/abs/2005.11401,Original RAG paper. Defines retrieve-then-generate paradigm and demonstrates RAG produces more factual language than parametric-only baselines.
S23,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection","Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi",2023,Empirical study (peer-reviewed),ICLR 2024,https://arxiv.org/abs/2310.11511,Introduces reflection tokens for adaptive retrieval and self-critique. Significant gains in factuality and citation accuracy over standard RAG.
S24,Retrieval-Augmented Generation for Large Language Models: A Survey,"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang",2024,Survey,arXiv preprint (arXiv:2312.10997),https://arxiv.org/abs/2312.10997,"Comprehensive RAG survey covering Naive, Advanced, and Modular RAG paradigms. Evaluation frameworks and benchmarks."
S25,Towards Verifiable Text Generation with Generative Agent,(AAAI 2025 proceedings),2025,Empirical study (peer-reviewed),AAAI 2025,https://ojs.aaai.org/index.php/AAAI/article/view/34599,Agent-based citation generation achieving +154.7% citation quality improvement on ALCE. Retrieval of best-matched demonstrations improves citation recall and precision.
S26,Faithfulness Metric — DeepEval Documentation,Confident AI (DeepEval team),2024,Technical documentation,deepeval.com,https://deepeval.com/docs/metrics-faithfulness,Practical faithfulness evaluation implementation using LLM-as-a-judge. QAG scorer methodology and code examples.
S27,Faithfulness — RAGAS Documentation,RAGAS team,2024,Technical documentation,docs.ragas.io,https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/,Faithfulness scoring using claim extraction and NLI verification. Includes Vectara HHEM-2.1-Open classifier integration.
S28,LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide,Confident AI,2024,Technical guide,confident-ai.com,https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation,"Comprehensive guide to RAG evaluation metrics covering faithfulness, answer relevance, context precision/recall, and hallucination metrics."
